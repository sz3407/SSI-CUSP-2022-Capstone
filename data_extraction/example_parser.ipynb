{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pdfminer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "YEARS_STRING = ['1990', '1991', '1992', '1994', '1995', '1996', '1997', '1998', \n",
    "                '1999', '2000', '2004', '2005', '2006', '2007', '2008', '2011']\n",
    "\n",
    "YEARS = [1990, 1991, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2004, 2005, 2006, 2007, 2008, 2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in state_constants as dataframe from external file\n",
    "STATE_CONSTANTS = pd.read_csv(\"util/state_constants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load errata_list constant from file\n",
    "%load util/errata_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning functions\n",
    "\n",
    "#removes any new line characters from string and drops right-end spaces\n",
    "def drop_new_lines_plus_strip(string):\n",
    "    return re.sub('\\n+', '', string).rstrip()\n",
    "\n",
    "#removes any dollar sign characters or commas plus new lines from string\n",
    "def drop_dollar_sign_and_comma(string):\n",
    "    return re.sub('\\$|,|\\n', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: \n",
    "# - raw_text: body of pdf report file as String\n",
    "#return: list of State Payment Table pages text\n",
    "\n",
    "# Step 1 -> convert contents of pdf report into separate payment pages.\n",
    "def do_preprocessing(raw_text):\n",
    "    \n",
    "    #create translator to strip away '\\0xC' (formfeed) character in converted text\n",
    "    drop_formfeed_translator = str.maketrans('', '', '\\f')\n",
    "    working_text = raw_text.translate(drop_formfeed_translator)\n",
    "    #drop any duplicate spaces\n",
    "    working_text = re.sub(' +', ' ', working_text)\n",
    "\n",
    "    #replace known misrecognition(s) in converted text\n",
    "    for idx, ele in enumerate(ERRATA_LIST):\n",
    "        for jdx, err in enumerate(ele[1]):\n",
    "            working_text = re.sub(err, ele[0], working_text)\n",
    "    \n",
    "    return working_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: \n",
    "# - working_text: body of pdf report file as String after preprocessing\n",
    "#return: list of States without optional supplementation, list of States with program\n",
    "\n",
    "# Step 2 -> iterate through pdf pages and capture States without optional program.\n",
    "def extract_no_program_states(working_text):\n",
    "\n",
    "    #define Regex for capturing separate pages within pdf\n",
    "    pages_pattern = r\"(Digitized by Google).*?(?=Digitized by Google)\"\n",
    "    __matches = [x.group() for x in re.finditer(pages_pattern, working_text, flags=re.M|re.S)]\n",
    "\n",
    "    # This string is present on all payment pages where state does not have optional program.\n",
    "    _tofind = \"State does not provide optional supplementation.\"\n",
    "\n",
    "    saved_pages = []\n",
    "    no_program_states = []\n",
    "\n",
    "    for ele in __matches:\n",
    "        #if page contains \"no-program\" string\n",
    "        if _tofind in ele:\n",
    "            saved_pages.append(re.sub('\\n', '', ele))\n",
    "\n",
    "    states_upper = STATE_CONSTANTS['full_name_upper'].tolist()\n",
    "            \n",
    "    for page in saved_pages:\n",
    "        #split contents of page into separate strings\n",
    "        temp_tokens = page.split()\n",
    "        for idx, token in enumerate(temp_tokens):\n",
    "            #joining \"West Virginia\"\n",
    "            if token == 'WEST':\n",
    "                token = 'WEST VIRGINIA'\n",
    "            #check for \"duplicate\" Virginia\n",
    "            if token == 'VIRGINIA':\n",
    "                if (temp_tokens[idx-1] == 'WEST'):\n",
    "                    token = '/'\n",
    "            #find token in states list\n",
    "            for state in states_upper:\n",
    "                if token == state.upper():\n",
    "                    no_program_states.append(state.upper())\n",
    "\n",
    "    #create list of states with program from difference of those with no program    \n",
    "    states_w_program = list(set(states_upper).difference(no_program_states))\n",
    "    states_w_program.sort()\n",
    "\n",
    "    return no_program_states, states_with_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: \n",
    "# - working_text: body of pdf report file as String after preprocessing\n",
    "#return: list of state payment pages as separate strings\n",
    "\n",
    "# Step 3 -> split pdf body into separate pages based on State Payment Table heading.\n",
    "def split_payment_pages(working_text):\n",
    "    \n",
    "    #define Regex for capturing State Payment Tables\n",
    "    pattern_payment_levels = r\"((PAYMENT LEVELS)|(Optional supplement amount)).*?(?=Digitized)\"\n",
    "    #split text into separate payment pages by state\n",
    "    raw_payment_pages = [x.group() for x in re.finditer(pattern_payment_levels, working_text, flags=re.M|re.S)]\n",
    "    \n",
    "    return raw_payment_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: \n",
    "# - raw_payment_pages: body of pdf report file as String\n",
    "# - table_data: empty list -> contents of payment table for page\n",
    "# - footnotes: empty list -> footnotes for table from bottom of page (String)\n",
    "# - temp: used for temporary swapping inside function\n",
    "#return: void, mutates input lists\n",
    "\n",
    "#separator = 'STATE ASSISTANCE FOR SPECIAL NEEDS'\n",
    "# This text was a consistent heading in the report(s) following the conclusion of the State Payment Table, however \n",
    "#   it may or may not be present on the same page depending on the length of the specific table.\n",
    "\n",
    "#pattern_footnotes = r\"(Unless|Blind ind|Add).*\"\n",
    "# Regex for capturing footnotes using prefixes section text for all state tables.\n",
    "\n",
    "#pattern_payments = r\"(PAYMENT LEVELS).*(?=Unles)\"\n",
    "# Regex for capturing payment tables and bounding with footnotes start for all state tables.\n",
    "\n",
    "# Step 4 -> convert separate payment pages into lists of raw table data + table footnotes.\n",
    "def parse_pages(raw_payment_pages, table_data, footnotes, temp):\n",
    "    for idx, ele in enumerate(raw_payment_pages):\n",
    "        splitted = ele.split(separator, 1)\n",
    "        #if separator present\n",
    "        if (len(splitted) > 1):\n",
    "            table_data.append(splitted[0])\n",
    "            temp.append(splitted[1])\n",
    "        else:\n",
    "            table_data.append(splitted[0])\n",
    "            temp.append(splitted[0])\n",
    "\n",
    "    #grab footnotes\n",
    "    for idx, ele in enumerate(temp):\n",
    "        footnotes.append([x.group() for x in re.finditer(pattern_footnotes, ele, flags=re.S)])\n",
    "\n",
    "    #grab table data\n",
    "    for idx, ele in enumerate(table_data):\n",
    "        temp = [x.group() for x in re.finditer(pattern_payments, ele, flags=re.S)]\n",
    "        if (len(temp) >= 1):\n",
    "            table_data[idx] = temp[0]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: none\n",
    "#return: void, mutates local structures\n",
    "\n",
    "#pattern_liv_arr = r\"(Rec|Living (in|with|alone|I)|Req|(Un)*((L|l)ice)|Medicaid facility|Inde|Nonm|Disabled \\w|(Care)* Home|Individual w|Adult|Foster|Comm|Long-|Small|Large.*/Res|Domiciliary( |-)(C|c)are|Specialized s|Semi-i|Room|Hotel-|Family|Resid|In-|Personal|Caret|Flat|Cost reimb|DOMCARE|Shared|Home|Aid to|Group home|Child|Trans|Certi|Center|Congre|Blind in|Rest|Supervised|Custo|House|Private|Shelter).*?(?=(\\d{2,}\\.\\d{2})|[$\\\"\\.:•A-Z\\('~]|([\\d]{1,2} )|([\\d!]/))\"\n",
    "# Regex for capturing living arrangements defined and covered across all state tables.\n",
    "\n",
    "#pattern_benef = r\"((N/A)|((\\$)*[\\d]{1,3}\\.[\\d]{2})|((\\$)*[\\d],[\\d]{3}\\.[\\d]{2}))\"\n",
    "# Regex for capturing benefit amounts from within table body text for matching living arrangements.\n",
    "\n",
    "# Step 5 -> split raw table data into lists of living arrangements and benefit amounts.\n",
    "def parse_table_data():\n",
    "    for idx, ele in enumerate(table_data):\n",
    "        liv_arr_matches = [x.group() for x in re.finditer(pattern_liv_arr, table_data[idx], flags=re.S)]\n",
    "        benef_matches = [x.group() for x in re.finditer(pattern_benef, table_data[idx], flags=re.S)]\n",
    "\n",
    "        mut_liv_arrs = []\n",
    "        mut_benefs = []\n",
    "\n",
    "        for jdx, match in enumerate(liv_arr_matches):\n",
    "            mut_liv_arrs.append(drop_new_lines_plus_strip(match))\n",
    "\n",
    "        for jdx, match in enumerate(benef_matches):\n",
    "            mut_benefs.append(drop_dollar_sign_and_comma(match))\n",
    "\n",
    "        #pushing data to local lists\n",
    "        liv_arrs.append(mut_liv_arrs)\n",
    "        benefs.append(mut_benefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6 -> manual fixes\n",
    "def manual_fix(report_year):\n",
    "\n",
    "    #create path to files\n",
    "    lahc = report_year + \"_lahc.py\"\n",
    "    bhc = report_year + \"_bhc.py\"\n",
    "    \n",
    "    #load and execute hc files for report year\n",
    "    %load util/liv_arr_hc/lahc\n",
    "    %load util/benef_hc/bhc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: none\n",
    "#return: void, mutates local structures\n",
    "\n",
    "# Step 7 -> modify living arrangements list to include state defined subcategories.\n",
    "def add_subcats():\n",
    "    for idx, state in enumerate(liv_arrs):\n",
    "\n",
    "        #check if state has any adjustments\n",
    "        if (len(LIVING_ARRANGEMENT_ADJUSTMENTS_BASE[idx]) > 0):\n",
    "            temp_adjustments = []\n",
    "            #loop through arrangements list\n",
    "            for jdx in range(len(state)):\n",
    "                made_adj = False\n",
    "                #loop through potential adjustments\n",
    "                for kdx, adj_list in enumerate(LIVING_ARRANGEMENT_ADJUSTMENTS_BASE[idx]):\n",
    "                    #check if adjustment index matches arrangement index\n",
    "                    if (adj_list[0] == jdx):\n",
    "                        #flag adjustment made (avoid copies)\n",
    "                        made_adj = True\n",
    "                        #loop through actual adjustments\n",
    "                        for mdx, add_on in enumerate(adj_list[1]):\n",
    "                            temp_adjustments.append([state[jdx], add_on])\n",
    "                if (not made_adj):\n",
    "                    temp_adjustments.append([state[jdx], ''])\n",
    "            adj_liv_arrs.append(temp_adjustments)\n",
    "        #if no adjustments, push as is\n",
    "        else:\n",
    "            temp_adjustments = []\n",
    "            for jdx in range(len(state)):\n",
    "                temp_adjustments.append([state[jdx], ''])\n",
    "            adj_liv_arrs.append(temp_adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: none\n",
    "#return: void, mutates local structures\n",
    "\n",
    "#Step 8 -> create list of `State_Obj` containing covered living arrangments and benefits.\n",
    "def build_state_objs():\n",
    "    for idx, ele in enumerate(STATES_W_PROGRAM):\n",
    "        state_objs.append({\n",
    "            'state': ele,\n",
    "            'liv_arrs': adj_liv_arrs[idx],\n",
    "            'benefs': benefs[idx]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index_mappings = {\n",
    "    'A':(0,4),\n",
    "    'B':(4,8),\n",
    "    'C':(8,12),\n",
    "    'D':(12,16),\n",
    "    'E':(16,20),\n",
    "    'F':(20,24),\n",
    "    'G':(24,28),\n",
    "    'H':(28,32),\n",
    "    'I':(32,36),\n",
    "    'J':(36,40),\n",
    "    'K':(40,44),\n",
    "    'L':(44,48),\n",
    "    'M':(48,52),\n",
    "    'N':(52,56),\n",
    "    'O':(56,60)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(string, state_idx):\n",
    "    tokens_ = list(set(string))\n",
    "    tokens_.sort()\n",
    "\n",
    "    token_ranges = []\n",
    "\n",
    "    for token in tokens_:\n",
    "        token_ranges.append([token, token_to_index_mappings[token][0], token_to_index_mappings[token][1]])\n",
    "\n",
    "    idx_list = []\n",
    "    fir_tkn = token_ranges[0][0]\n",
    "    lst_tkn = token_ranges[len(token_ranges) - 1][0]    \n",
    "    benef_slice = benefs[state_idx][token_to_index_mappings[fir_tkn][0] : token_to_index_mappings[lst_tkn][1]]\n",
    "\n",
    "\n",
    "    x_token_idxs, y_token_idxs = [], []\n",
    "    x_token = token_ranges[0][0]\n",
    "    y_token = token_ranges[1][0]\n",
    "\n",
    "    for idx, char in enumerate(string):\n",
    "        if (char == x_token):\n",
    "            x_token_idxs.append(idx)\n",
    "        else:\n",
    "            y_token_idxs.append(idx)\n",
    "\n",
    "    joined_idxs = x_token_idxs + y_token_idxs\n",
    "\n",
    "    benef_slice = [benef_slice[idx] for idx in joined_idxs]\n",
    "\n",
    "    return [benef_slice, token_to_index_mappings[fir_tkn][0], token_to_index_mappings[lst_tkn][1], tokens_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payment_pattern_parser(pattern_str):\n",
    "\n",
    "    token_buffer = ''\n",
    "    tokens = []\n",
    "\n",
    "    for char in pattern_str:\n",
    "        if (not char.isspace()):\n",
    "            token_buffer = token_buffer + char\n",
    "        else:\n",
    "            tokens.append(token_buffer)\n",
    "            token_buffer = ''\n",
    "\n",
    "    tokens.append(token_buffer)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_matcher(tokens):\n",
    "\n",
    "    instruction_list = []\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        x_type_pattern = r\"(?<![A-Z_*])[A-Z](?![A-Z])\" \n",
    "        xy_type_pattern = r\"\\([A-Z]{2}\\)\"\n",
    "        xyz_type_pattern = r\"\\([A-Z]{3}\\)\"\n",
    "        wxyz_type_pattern = r\"\\([A-Z]{4}\\)\"\n",
    "        vwxyz_type_pattern = r\"\\([A-Z]{5}\\)\"\n",
    "        uvwxyz_type_pattern = r\"\\([A-Z]{6}\\)\"\n",
    "        shuffle_rows_pattern = r\"%[A-Z]*%\"\n",
    "        \n",
    "        if re.search(x_type_pattern, token): #A\n",
    "            instruction_list.append(('single_split', token_to_index_mappings[token[0]][0], token_to_index_mappings[token[0]][1]))\n",
    "        elif re.search(xy_type_pattern, token): #(AB)\n",
    "            first_arr = token[1:2]\n",
    "            second_arr = token[2:3]\n",
    "            instruction_list.append(('swap_xy', token_to_index_mappings[first_arr][0], token_to_index_mappings[second_arr][1]))\n",
    "        elif re.search(xyz_type_pattern, token): #(ABC)\n",
    "            first_arr = token[1:2]\n",
    "            third_arr = token[3:4]\n",
    "            instruction_list.append(('swap_xyz', token_to_index_mappings[first_arr][0], token_to_index_mappings[third_arr][1]))\n",
    "        elif re.search(wxyz_type_pattern, token): #(ABCD)\n",
    "            first_arr = token[1:2]\n",
    "            fourth_arr = token[4:5]\n",
    "            instruction_list.append(('swap_wxyz', token_to_index_mappings[first_arr][0], token_to_index_mappings[fourth_arr][1]))\n",
    "        elif re.search(vwxyz_type_pattern, token): #(ABCDE)\n",
    "            first_arr = token[1:2]\n",
    "            fifth_arr = token[5:6]\n",
    "            instruction_list.append(('swap_vwxyz', token_to_index_mappings[first_arr][0], token_to_index_mappings[fifth_arr][1]))\n",
    "        elif re.search(uvwxyz_type_pattern, token): #(ABCDEF)\n",
    "            first_arr = token[1:2]\n",
    "            sixth_arr = token[6:7]\n",
    "            instruction_list.append(('swap_uvwxyz', token_to_index_mappings[first_arr][0], token_to_index_mappings[sixth_arr][1]))\n",
    "        elif re.search(shuffle_rows_pattern, token): #%...%\n",
    "            string = token[1:-1]\n",
    "            instruction_list.append(('shuffle', string))\n",
    "        else:\n",
    "            print(token)\n",
    "            \n",
    "    return instruction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpreter(_instruction_list, states_idx):\n",
    "    sorted_payments = []\n",
    "    for idx, instr in enumerate(_instruction_list):\n",
    "        if (instr[0] == 'single_split'):\n",
    "            sorted_payments += (single_split(state_objs[states_idx]['benefs'][instr[1]:instr[2]]))\n",
    "        elif (instr[0] == 'swap_xy'):\n",
    "            #swap_abc(states_objs[18]['payments'][0:12])\n",
    "            sorted_payments += (swap(state_objs[states_idx]['benefs'][instr[1]:instr[2]], 2))\n",
    "        elif (instr[0] == 'swap_xyz'):\n",
    "            sorted_payments += (swap(state_objs[states_idx]['benefs'][instr[1]:instr[2]], 3))\n",
    "        elif (instr[0] == 'swap_wxyz'):\n",
    "            sorted_payments += (swap(state_objs[states_idx]['benefs'][instr[1]:instr[2]], 4))\n",
    "        elif (instr[0] == 'swap_vwxyz'):\n",
    "            sorted_payments += (swap(state_objs[states_idx]['benefs'][instr[1]:instr[2]], 5))\n",
    "        elif (instr[0] == 'swap_uvwxyz'):\n",
    "            sorted_payments += (swap(state_objs[states_idx]['benefs'][instr[1]:instr[2]], 6))\n",
    "        elif (instr[0] == 'shuffle'):\n",
    "            shuffle_ret = shuffle(instr[1], states_idx)\n",
    "            _splits = single_split(shuffle_ret[0])\n",
    "            for row in _splits:\n",
    "                sorted_payments += [row]\n",
    "\n",
    "    return sorted_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_split(_list):\n",
    "    return ([_list[i:i+4] for i in range(0, len(_list), 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(_list, count):\n",
    "\n",
    "    u_list = []\n",
    "    v_list = []\n",
    "    w_list = []\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "\n",
    "    for idx, ele in enumerate(_list):\n",
    "        \n",
    "        # (xy) swap\n",
    "        if (count == 2):\n",
    "            if (idx % 2 == 0):\n",
    "                x_list.append(ele)\n",
    "            else:\n",
    "                y_list.append(ele)\n",
    "        \n",
    "        # (xyz) swap\n",
    "        elif (count == 3):\n",
    "            if (idx % 3 == 0):\n",
    "                x_list.append(ele)\n",
    "            elif (idx % 3 == 1):\n",
    "                y_list.append(ele)\n",
    "            else:\n",
    "                z_list.append(ele)\n",
    "        \n",
    "        # (wxyz) swap\n",
    "        elif (count == 4):\n",
    "            if (idx % 4 == 0):\n",
    "                w_list.append(ele)\n",
    "            elif (idx % 4 == 1):\n",
    "                x_list.append(ele)\n",
    "            elif (idx % 4 == 2):\n",
    "                y_list.append(ele)\n",
    "            else:\n",
    "                z_list.append(ele)\n",
    "        \n",
    "        # (vwxyz) swap\n",
    "        elif (count == 5):\n",
    "            if (idx % 5 == 0):\n",
    "                v_list.append(ele)\n",
    "            elif (idx % 5 == 1):\n",
    "                w_list.append(ele)\n",
    "            elif (idx % 5 == 2):\n",
    "                x_list.append(ele)\n",
    "            elif (idx % 5 == 3):\n",
    "                y_list.append(ele)\n",
    "            else:\n",
    "                z_list.append(ele)\n",
    "                \n",
    "        # (uvwxyz) swap\n",
    "        elif (count == 6):\n",
    "            if (idx % 6 == 0):\n",
    "                u_list.append(ele)\n",
    "            elif (idx % 6 == 1):\n",
    "                v_list.append(ele)\n",
    "            elif (idx % 6 == 2):\n",
    "                w_list.append(ele)\n",
    "            elif (idx % 6 == 3):\n",
    "                x_list.append(ele)\n",
    "            elif (idx % 6 == 4):\n",
    "                y_list.append(ele)\n",
    "            else:\n",
    "                z_list.append(ele)\n",
    "                \n",
    "    if (count == 2):\n",
    "        return single_split(x_list) + single_split(y_list)\n",
    "    elif (count == 3):\n",
    "        return single_split(x_list) + single_split(y_list) + single_split(z_list)\n",
    "    elif (count == 4):\n",
    "        return single_split(w_list) + single_split(x_list) + single_split(y_list) + single_split(z_list)\n",
    "    elif (count == 5):\n",
    "        return single_split(v_list) + single_split(w_list) + single_split(x_list) + single_split(y_list) + single_split(z_list)\n",
    "    else:\n",
    "        return single_split(u_list) + single_split(v_list) + single_split(w_list) + single_split(x_list) + single_split(y_list) + single_split(z_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: none\n",
    "#return: void, mutates local structures\n",
    "\n",
    "#Step 9 -> transform benefits list to list-of-list format matching table structure.\n",
    "def parse_payments():\n",
    "    for idx, ele in enumerate(state_objs):\n",
    "        #ignore unique implementation states\n",
    "        if (not ele['state'] in troublesome_states):\n",
    "            ttokens = payment_pattern_parser(STATE_PAYMENT_PATTERNS_BASE[idx][1])\n",
    "            iinstr_list = token_matcher(ttokens)\n",
    "            ssorted_benef = interpreter(iinstr_list, idx)\n",
    "            state_payments.append(ssorted_benef)\n",
    "        else:\n",
    "            state_payments.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load liv_arr_subcat_base from outside file\n",
    "#\n",
    "# LIVING_ARRANGEMENT_ADJUSTMENTS_BASE\n",
    "%load util/subcats/LIVING_ARRANGEMENT_ADJUSTMENTS_BASE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load benefs_base_patterns from outside file\n",
    "#\n",
    "# STATE_PAYMENT_PATTERNS_BASE\n",
    "%load util/benef_patterns/STATE_PAYMENT_PATTERNS_BASE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns\n",
    "pattern_liv_arr = r\"(Rec|Living (in|with|alone|I)|Req|(Un)*((L|l)ice)|Medicaid facility|Inde|Nonm|Disabled \\w|(Care)* Home|Individual w|Adult|Foster|Comm|Long-|Small|Large.*/Res|Domiciliary( |-)(C|c)are|Specialized s|Semi-i|Room|Hotel-|Family|Resid|In-|Personal|Caret|Flat|Cost reimb|DOMCARE|Shared|Home|Aid to|Group home|Child|Trans|Certi|Center|Congre|Blind in|Rest|Supervised|Custo|House|Private|Shelter).*?(?=(\\d{2,}\\.\\d{2})|[$\\\"\\.:•A-Z\\('~]|([\\d]{1,2} )|([\\d!]/))\"\n",
    "pattern_benef = r\"((N/A)|((\\$)*[\\d]{1,3}\\.[\\d]{2})|((\\$)*[\\d],[\\d]{3}\\.[\\d]{2}))\"\n",
    "separator = 'STATE ASSISTANCE FOR SPECIAL NEEDS'\n",
    "pattern_footnotes = r\"(Unless|Blind ind|Add).*\"\n",
    "pattern_payments = r\"(PAYMENT LEVELS).*(?=Unles)\"\n",
    "pattern_footnote_benefs = r\"\\$[\\d]{2}\"\n",
    "\n",
    "for idx, year_str in enumerate(YEARS_STRING):\n",
    "    \n",
    "    #print(\"now working with \" + year_str + \" year ------------\")\n",
    "    \n",
    "    #local structures\n",
    "    table_data, footnotes, temp = [], [], []\n",
    "    liv_arrs = []\n",
    "    benefs = []\n",
    "    adj_liv_arrs = []\n",
    "    state_objs = []\n",
    "    state_payments = []\n",
    "    \n",
    "    #import data\n",
    "    raw_text = extract_text(\"ssi_reports/ssi_\" + year_str + \"s.pdf\")\n",
    "    \n",
    "    # Step 1\n",
    "    working_text = do_preprocessing(raw_text)\n",
    "    # Step 2\n",
    "    NO_PROGRAM_STATES, STATES_W_PROGRAM = extract_no_program_states(working_text)\n",
    "    # Step 3\n",
    "    raw_payment_pages = split_payment_pages(working_text)\n",
    "    # Step 4\n",
    "    parse_pages(raw_payment_pages, table_data, footnotes, temp)\n",
    "    \n",
    "    #if necessary, clean duplicate or run-on pages\n",
    "    \n",
    "    # Step 5\n",
    "    parse_table_data()\n",
    "    # Step 6\n",
    "    manual_fix(YEARS_STRING)\n",
    "\n",
    "    #modify for current year\n",
    "    _path = \"subcats_\" + year_str + \".py\"\n",
    "    %load util/subcats/_path\n",
    "\n",
    "    # Step 7\n",
    "    add_subcats()\n",
    "    # Step 8\n",
    "    build_state_objs()\n",
    "\n",
    "    #modify for current year\n",
    "    _path = \"patt_\" + year_str + \".py\"\n",
    "    PATTERN_ADJUSTMENTS = %load util/benef_patterns/_path\n",
    "\n",
    "    for idx, replacement in enumerate(PATTERN_ADJUSTMENTS):\n",
    "        for jdx, pattern in enumerate(STATE_PAYMENT_PATTERNS_BASE):\n",
    "            if (replacement[0] == pattern[0]):\n",
    "                STATE_PAYMENT_PATTERNS_BASE[jdx] = replacement\n",
    "\n",
    "    # Step 9\n",
    "    parse_payments()\n",
    "    \n",
    "    column_names = [\"year\", \"state\", \"liv_arr\", \"sub_cat\", \"combn_indv\", \"combn_cpl\", \"state_indv\", \"state_cpl\"]\n",
    "    liv_arr_x_benef_df = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    for idx, ele in enumerate(STATES_W_PROGRAM):\n",
    "        for num_rows in range(len(adj_liv_arrs[idx])):\n",
    "            liv_arr_x_benef_df.loc[len(liv_arr_x_benef_df.index)] = [year_str, ele, \n",
    "                                                    adj_liv_arrs[idx][num_rows][0],\n",
    "                                                    adj_liv_arrs[idx][num_rows][1],\n",
    "                                                    state_payments[idx][num_rows][0],\n",
    "                                                    state_payments[idx][num_rows][1],\n",
    "                                                    state_payments[idx][num_rows][2],\n",
    "                                                    state_payments[idx][num_rows][3]]\n",
    "    \n",
    "    liv_arr_x_benef_df = liv_arr_x_benef_df.replace({'combn_indv':{'N/A':'0'}, \n",
    "                                                     'combn_cpl':{'N/A':'0'},\n",
    "                                                     'state_indv':{'N/A':'0'},\n",
    "                                                     'state_cpl':{'N/A':'0'}})\n",
    "    \n",
    "    liv_arr_x_benef_df = liv_arr_x_benef_df.astype({'combn_indv':'float64', 'combn_cpl':'float64', \n",
    "                                                    'state_indv':'float64', 'state_cpl':'float64'})\n",
    "    \n",
    "    liv_arr_x_benef_df.to_csv('_' + year_str + '_liv_arr_x_benef.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing notebook view\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#styling output table in notebook view\n",
    "\n",
    "# final = liv_arr_x_benef_df.style\n",
    "# final.set_table_styles([{\n",
    "#     'selector': 'td', 'props': [\n",
    "#         ('font-size', '10pt'),\n",
    "#         ('border-style', 'solid'),\n",
    "#         ('border-width', '1px'),\n",
    "#         ('border-color', 'black')]}])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
